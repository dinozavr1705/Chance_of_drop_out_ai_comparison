                                  Почему результаты этих 2 нейронок различаются
1)Разное количество нейронов и слоев:1-ая нейронка – 4-16-1,а 2 – 4-32-16-8-1

2)Разные функции активации:1-ая нейронка – ReLU & Sigmoid, 2-ая нейронка – leaky ReLU & Sigmoid

3)Разное количество эпох:1-ая нейронка – 200,2-ая – 2000

                                                  Вывод
1)Большее количество слоев и нейронов повышают точность,но это требует больше времени 

2)Количество эпох влияет на результат,однако слишком большое их количество может привести к переобучению

3)Обычный RelU хоть и быстрее leaky ReLU,но есть шанс того,что нейрон умрет,а leaky ReLU решает эту проблему

4)Не смотря на то,что маленький batch увеличивает производительность и ускоряет адаптацию модели,он создает шум ,т.е. градиент может выйти из локальных минимумов,высокий же batch делает обучение более стабильным

